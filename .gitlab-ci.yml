variables:
  PIPELINE_TOOLBOX_IMAGE: "registry.gitlab.com/northern.tech/mender/mender-test-containers:aws-k8s-v1-master"
  EKS_CLUSTER_NAME: "helmci-${CI_PIPELINE_ID}"

stages:
  - build
  - test
  - publish

include:
  - project: 'Northern.tech/Mender/mendertesting'
    file: '.gitlab-ci-check-commits-signoffs.yml'
  - project: 'Northern.tech/Mender/mendertesting'
    file: '.gitlab-ci-check-license.yml'
  - project: 'Northern.tech/Mender/mendertesting'
    file: '.gitlab-ci-github-status-updates.yml'

.set_eks_helmci_vars: &set_eks_helmci_vars
  - export AWS_ACCESS_KEY_ID=$CI_EKSCTL_ACCESS_KEY_ID_TEST
  - export AWS_SECRET_ACCESS_KEY=$CI_EKSCTL_AWS_SECRET_ACCESS_KEY_TEST
  - export AWS_DEFAULT_REGION=eu-central-1

build:setup_eks_cluster:
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: .pre
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  before_script:
  script:
    - *set_eks_helmci_vars
    - |
      echo "INFO - Temporary EKS cluster setup"
      eksctl create cluster \
        --name ${EKS_CLUSTER_NAME} \
        --tags "env=helm-ci-test" \
        --asg-access \
        --spot \
        --instance-types m6a.xlarge,t3.xlarge,m5d.xlarge,m6i.xlarge \
        --nodes ${CI_EKS_NUMBER_OF_NODES:-5} 
    - echo "INFO - assigning SSO roles for debugging failed tests:"
    - eksctl create iamidentitymapping --cluster ${EKS_CLUSTER_NAME} --arn arn:aws:iam::017659451055:role/AWSReservedSSO_AdministratorAccess_d80c0803237d713a --username "admin:{{SessionName}}" --group "system:masters" --no-duplicate-arns
    - eksctl create iamidentitymapping --cluster ${EKS_CLUSTER_NAME} --arn arn:aws:iam::017659451055:role/AWSReservedSSO_PowerUserAccess_28a0f819b31196fb --username "admin:{{SessionName}}" --group "system:masters" --no-duplicate-arns
    - eksctl create iamidentitymapping --cluster ${EKS_CLUSTER_NAME} --arn arn:aws:iam::017659451055:role/AWSReservedSSO_EKSAccess_8c2574e21d9fc21c --username "admin:{{SessionName}}" --group "system:masters" --no-duplicate-arns
    - eksctl utils associate-iam-oidc-provider --cluster ${EKS_CLUSTER_NAME} --approve
    - eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster ${EKS_CLUSTER_NAME} --role-name AmazonEKS_EBS_CSI_DriverRole --role-only --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve
    - eksctl create addon --name aws-ebs-csi-driver --cluster ${EKS_CLUSTER_NAME} --service-account-role-arn arn:aws:iam::017659451055:role/AmazonEKS_EBS_CSI_DriverRole --force
    - echo "DEBUG - kubectl cluster version:"
    - kubectl version

build:
  stage: build
  image: debian:buster
  before_script:
    - apt-get update -y
    - apt-get install -y curl make
    - curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | DESIRED_VERSION="v3.8.2" bash
  script:
    - make lint
    - make package
  artifacts:
    expire_in: 2w
    paths:
      - mender-*.tgz

.get_kubectl_and_tools: &get_kubectl_and_tools
  # Install kubectl
  - apt update && apt install -yyq curl
  - curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.6/bin/linux/amd64/kubectl
  - chmod +x ./kubectl
  - mv ./kubectl /usr/local/bin/kubectl
  # Install AWS CLI and aws-iam-authenticator
  - apt install -yyq awscli
  - curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator
  - chmod +x ./aws-iam-authenticator
  - mv ./aws-iam-authenticator /usr/local/bin/aws-iam-authenticator
  # Install kubectx
  - apt install -yyq kubectx

.setup_eks_cluster_staging: &setup_eks_cluster_staging
  # Configure AWS CLI for staging cluster
  - export AWS_ACCESS_KEY_ID=$CI_JOBS_AWS_ACCESS_KEY_ID_STAGING
  - export AWS_SECRET_ACCESS_KEY=$CI_JOBS_AWS_SECRET_ACCESS_KEY_STAGING
  - export AWS_DEFAULT_REGION=$CI_JOBS_AWS_REGION_STAGING
  - aws eks --region $CI_JOBS_AWS_REGION_STAGING update-kubeconfig --name $CI_JOBS_AWS_EKS_CLUSTER_NAME_STAGING
  - kubectl config set-context --current --namespace=mender-helm-tests

.setup_s3_helm_chart_repo: &setup_s3_helm_chart_repo
  # Configure AWS CLI for the S3 repository
  - export AWS_ACCESS_KEY_ID=$S3_HELM_CHART_REPO_AWS_ACCESS_KEY_ID
  - export AWS_SECRET_ACCESS_KEY=$S3_HELM_CHART_REPO_AWS_SECRET_ACCESS_KEY
  - export AWS_DEFAULT_REGION=$S3_HELM_CHART_REPO_AWS_REGION
  - export AWS_S3_SSE=AES256

test:helm_chart_install:
  tags:
    - mender-qa-worker-generic-light
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: test
  needs: ["build:setup_eks_cluster"]
  dependencies:
    - build:setup_eks_cluster
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  variables:
    NAMESPACE: "mender-setup-test"
  before_script:
    - *set_eks_helmci_vars
    - |
      eksctl utils write-kubeconfig --cluster=${EKS_CLUSTER_NAME}
      kubectl create ns ${NAMESPACE}
      kubectl config set-context --current --namespace=${NAMESPACE}
    - |
      echo "DEBUG - get kubectl nodes"
      kubectl config current-context
      kubectl get nodes
    - |
      echo "INFO - installing helm from scratch"
      tests/ci-deps-k8s.sh
  script:
    - tests/ci-make-deps.sh
    - |
      echo "INFO - installing mender"
      source ./tests/variables.sh
      helm upgrade -i mender \
        -f mender/values.yaml \
        -f tests/keys.yaml \
        -f tests/values-helmci.yaml \
        --wait \
        --timeout=${HELM_UPGRADE_TIMEOUT:-15m} \
        --set global.image.username=${REGISTRY_MENDER_IO_USERNAME} \
        --set global.image.password="${REGISTRY_MENDER_IO_PASSWORD}" \
        --set global.s3.AWS_ACCESS_KEY_ID="${MINIO_accessKey}" \
        --set global.s3.AWS_SECRET_ACCESS_KEY="${MINIO_secretKey}" \
        --namespace ${NAMESPACE} \
        ./mender || exit 3;
    - make test
  after_script:
    - echo "INFO - cleaning up to save resources"
    - helm delete -n ${NAMESPACE} mender 

test:helm_chart_install_sub_charts:
  tags:
    - mender-qa-worker-generic-light
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: test
  needs: ["build:setup_eks_cluster"]
  dependencies:
    - build:setup_eks_cluster
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  variables:
    NAMESPACE: "mender-setup-test-sub-charts"
    INSTALL_MINIO_DEP_ONLY: "true"
  before_script:
    - *set_eks_helmci_vars
    - |
      eksctl utils write-kubeconfig --cluster=${EKS_CLUSTER_NAME}
      kubectl create ns ${NAMESPACE}
      kubectl config set-context --current --namespace=${NAMESPACE}
    - |
      echo "DEBUG - get kubectl nodes"
      kubectl config current-context
      kubectl get nodes
    - |
      echo "INFO - installing helm from scratch"
      tests/ci-deps-k8s.sh
  script:
    - tests/ci-make-deps.sh $INSTALL_MINIO_DEP_ONLY
    - |
      echo "INFO - installing mender"
      source ./tests/variables.sh
      helm upgrade -i mender \
        -f mender/values.yaml \
        -f tests/keys.yaml \
        -f tests/values-helmci.yaml \
        -f tests/values-helmci-internal-backing-services.yaml \
        --wait \
        --timeout=${HELM_UPGRADE_TIMEOUT:-15m} \
        --set global.image.username=${REGISTRY_MENDER_IO_USERNAME} \
        --set global.image.password="${REGISTRY_MENDER_IO_PASSWORD}" \
        --set global.s3.AWS_ACCESS_KEY_ID="${MINIO_accessKey}" \
        --set global.s3.AWS_SECRET_ACCESS_KEY="${MINIO_secretKey}" \
        --namespace ${NAMESPACE} \
        ./mender || exit 3;
    - make test
  after_script:
    - echo "INFO - cleaning up resources"
    - helm delete -n ${NAMESPACE} mender 
 
# this tests an update from the stable 3.3 to the current branch
test:helm_chart_upgrade_from_last_tls_to_3_6:
  tags:
    - mender-qa-worker-generic-light
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: test
  needs: ["build:setup_eks_cluster"]
  dependencies:
    - build:setup_eks_cluster
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  variables:
    NAMESPACE: "mender-upgrade-test-last-tls"
  before_script:
    - *set_eks_helmci_vars
    - |
      eksctl utils write-kubeconfig --cluster=${EKS_CLUSTER_NAME}
      kubectl create ns ${NAMESPACE}
      kubectl config set-context --current --namespace=${NAMESPACE}
    - |
      echo "DEBUG - get kubectl nodes"
      kubectl config current-context
      kubectl get nodes
    - |
      echo "INFO - installing helm from scratch"
      tests/ci-deps-k8s.sh
  script:
    - tests/ci-make-deps.sh
    - |
      echo "INFO - installing mender tag ${MENDER_LTS_RELEASE_TAG:-mender-3.3} with helm chart version ${MENDER_LTS_HELM_VERSION:-5.0.1}"
      source ./tests/variables.sh
      helm upgrade -i mender \
        -f mender/values.yaml \
        -f tests/keys.yaml \
        -f tests/values-helmci.yaml \
        --wait \
        --timeout=${HELM_UPGRADE_TIMEOUT:-15m} \
        --set global.image.tag="${MENDER_LTS_RELEASE_TAG:-mender-3.3}" \
        --set global.image.username=${REGISTRY_MENDER_IO_USERNAME} \
        --set global.image.password="${REGISTRY_MENDER_IO_PASSWORD}" \
        --set global.s3.AWS_ACCESS_KEY_ID="${MINIO_accessKey}" \
        --set global.s3.AWS_SECRET_ACCESS_KEY="${MINIO_secretKey}" \
        --namespace ${NAMESPACE} \
        mender/mender --version ${MENDER_LTS_HELM_VERSION:-5.0.1} || exit 3;
    - bash tests/test-001-wait-for-pods-to-be-ready.sh
    - |
      echo "INFO - installing this mender"
      helm upgrade -i mender \
        -f mender/values.yaml \
        -f tests/keys.yaml \
        -f tests/values-helmci.yaml \
        --wait \
        --timeout=${HELM_UPGRADE_TIMEOUT:-15m} \
        --set global.image.tag="${MENDER_3_6_RELEASE_TAG:-mender-3.6}" \
        --set global.image.username=${REGISTRY_MENDER_IO_USERNAME} \
        --set global.image.password="${REGISTRY_MENDER_IO_PASSWORD}" \
        --set global.s3.AWS_ACCESS_KEY_ID="${MINIO_accessKey}" \
        --set global.s3.AWS_SECRET_ACCESS_KEY="${MINIO_secretKey}" \
        --namespace ${NAMESPACE} \
        ./mender || exit 3;
    - helm history mender
    - make test


# this tests an update from the stable 3.6 to the current branch
# has to be enabled after the first 3.6 stable publishing
.test:helm_chart_upgrade_from3.6:
  tags:
    - mender-qa-worker-generic-light
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: test
  needs: ["build:setup_eks_cluster"]
  dependencies:
    - build:setup_eks_cluster
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  variables:
    NAMESPACE: "mender-upgrade-test-3-6"
  before_script:
    - *set_eks_helmci_vars
    - |
      eksctl utils write-kubeconfig --cluster=${EKS_CLUSTER_NAME}
      kubectl create ns ${NAMESPACE}
      kubectl config set-context --current --namespace=${NAMESPACE}
    - |
      echo "DEBUG - get kubectl nodes"
      kubectl config current-context
      kubectl get nodes
    - |
      echo "INFO - installing helm from scratch"
      tests/ci-deps-k8s.sh
  script:
    - tests/ci-make-deps.sh
    - |
      echo "INFO - installing mender tag ${MENDER_3_6_RELEASE_TAG:-mender-3.6}"
      source ./tests/variables.sh
      helm upgrade -i mender \
        -f mender/values.yaml \
        -f tests/keys.yaml \
        -f tests/values-helmci.yaml \
        --wait \
        --timeout=${HELM_UPGRADE_TIMEOUT:-15m} \
        --set global.image.tag="${MENDER_3_6_RELEASE_TAG:-mender-3.6}" \
        --set global.image.username=${REGISTRY_MENDER_IO_USERNAME} \
        --set global.image.password="${REGISTRY_MENDER_IO_PASSWORD}" \
        --set global.s3.AWS_ACCESS_KEY_ID="${MINIO_accessKey}" \
        --set global.s3.AWS_SECRET_ACCESS_KEY="${MINIO_secretKey}" \
        --namespace mender-upgrade-test-3-6 \
        mender/mender || exit 3;
    - bash tests/test-001-wait-for-pods-to-be-ready.sh
    - |
      echo "INFO - installing this mender"
      helm upgrade -i mender \
        -f mender/values.yaml \
        -f tests/keys.yaml \
        -f tests/values-helmci.yaml \
        --wait \
        --timeout=${HELM_UPGRADE_TIMEOUT:-15m} \
        --set global.image.tag="${MENDER_3_6_RELEASE_TAG:-mender-3.6}" \
        --set global.image.username=${REGISTRY_MENDER_IO_USERNAME} \
        --set global.image.password="${REGISTRY_MENDER_IO_PASSWORD}" \
        --set global.s3.AWS_ACCESS_KEY_ID="${MINIO_accessKey}" \
        --set global.s3.AWS_SECRET_ACCESS_KEY="${MINIO_secretKey}" \
        --namespace mender-upgrade-test-3-6 \
        ./mender || exit 3;
    - helm history mender
    - make test

test:kubeconform_tests:
  tags:
    - mender-qa-worker-generic-light
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: test
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  before_script:
    - *set_eks_helmci_vars
  script:
    - |
      echo "INFO - running kubeconform tests:"
      while IFS= read -r version; do
        full_version=${version}.0
        echo "INFO - testing version ${full_version}"
        helm template mender/ \
          -f mender/values.yaml \
          -f tests/keys.yaml \
          -f tests/values-helmci.yaml \
          --kube-version ${full_version} \
        | kubeconform --kubernetes-version ${full_version}
      done < <(eksctl version -o json | jq -r '.EKSServerSupportedVersions[]')
    - | 
      echo "DEBUG - run failed kubeconform tests: this should fail"
      sed -i 's/apps\/v1/fakeapps\/v42/' mender/templates/gui-deploy.yaml
      while IFS= read -r version; do
        full_version=${version}.0
        echo "INFO - testing version ${full_version}"
        helm template mender/ \
          -f mender/values.yaml \
          -f tests/keys.yaml \
          -f tests/values-helmci.yaml \
          --kube-version ${full_version} \
        | kubeconform --kubernetes-version ${full_version} \
        || echo "INFO - test failed with version ${kubeversion}: this is expected"
      done < <(eksctl version -o json | jq -r '.EKSServerSupportedVersions[]')

publish:helm_chart_publishing:
  rules:
    - if: '$CI_COMMIT_TAG'
      changes:
        - mender/Chart.yaml
      when: manual
  stage: publish
  dependencies:
    - build
  image: debian:buster
  before_script:
    - apt-get update -y
    - apt-get install -y curl make git
    - curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | DESIRED_VERSION="v3.8.2" bash
    - *get_kubectl_and_tools
    - helm plugin install https://github.com/hypnoglow/helm-s3.git --version 0.14.0
    - *setup_s3_helm_chart_repo
  script:
    - helm repo add mender s3://${S3_HELM_CHART_REPO}
    - helm s3 push --acl="public-read" --relative --timeout=60s ./mender-*.tgz mender
    - aws cloudfront create-invalidation --distribution-id ${S3_HELM_CHART_CDN_DISTRIBUTION_ID} --paths "/*"

.eks_cleanup: &eks_cleanup
  image: ${PIPELINE_TOOLBOX_IMAGE}
  stage: .post
  allow_failure: true # can't find a way to avoid eks cleanup when no eks is setup, so let's allow to fail for now
  dependencies:
    - build:setup_eks_cluster
  rules:
    - if: '$CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "master-next" || $RUN_HELM_CHART_INSTALL == "true" || $CI_COMMIT_TAG'
  before_script:
    - *set_eks_helmci_vars
  script:
    - echo "INFO - deleting the temporary EKS cluster"
    - eksctl delete cluster -n ${EKS_CLUSTER_NAME}

cleanup_eks_cluster:failed:
  when: on_failure
  <<: *eks_cleanup

cleanup_eks_cluster:success:
  when: on_success
  <<: *eks_cleanup

cleanup_eks_cluster:failed:manual:
  when: manual
  <<: *eks_cleanup

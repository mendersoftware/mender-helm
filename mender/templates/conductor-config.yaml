{{- if .Values.conductor.enabled  }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: conductor-appconfig
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/name: conductor-appconfig
    app.kubernetes.io/managed-by: {{ .Release.Service }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: conductor
    app.kubernetes.io/part-of: mender
    helm.sh/chart: "{{ .Chart.Name }}"
data:

  # logging config
  log4j.properties: |
    log4j.rootLogger=INFO, A1

    # A1 is set to be a ConsoleAppender.
    log4j.appender.A1=org.apache.log4j.ConsoleAppender

    # A1 uses PatternLayout.
    log4j.appender.A1.layout=org.apache.log4j.PatternLayout
    log4j.appender.A1.layout.ConversionPattern=[%d{ISO8601}] [%t] %-5p %c %x - %m%n

  # Configuration file for conductor server
  config.properties: |
    # Database persistence model.  Possible values are memory, redis, and dynomite.
    # If ommitted, the persistence used is memory
    #
    # memory : The data is stored in memory and lost when the server dies.  Useful for testing or demo
    # redis : non-Dynomite based redis instance
    # dynomite : Dynomite cluster.  Use this for HA configuration.

    {{- if .Values.conductor.env.REDIS_CLUSTER -}}
    db=redis_cluster
    {{- else -}}
    db=redis
    {{- end -}}

    # Dynomite Cluster details.
    # format is host:port:rack separated by semicolon
    workflow.dynomite.cluster.hosts={{ .Values.conductor.env.REDIS }}
    # Dynomite cluster name
    workflow.dynomite.cluster.name=dynomite

    # No. of threads allocated to dyno-queues (optional)
    queues.dynomite.threads=10
    # Non-quorum port used to connect to local redis.  Used by dyno-queues.
    # When using redis directly, set this to the same port as redis server
    # For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite.
    queues.dynomite.nonQuorum.port=6379

    # Namespace for the keys stored in Dynomite/Redis
    workflow.namespace.prefix=conductor_prod
    # Namespace prefix for the dyno queues
    workflow.namespace.queue.prefix=conductor_prod_queues

    # Transport address to elasticsearch
    workflow.elasticsearch.instanceType=EXTERNAL
    workflow.elasticsearch.url={{ .Values.conductor.env.ELASTICSEARCH }}
    # Name of the elasticsearch cluster
    workflow.elasticsearch.index.name={{ .Values.conductor.env.ELASTICSEARCH_INDEX }}

    # For single node dynomite, must be the same as 'rack'
    {{- if .Values.conductor.redis_rack -}}
    EC2_AVAILABILITY_ZONE={{ .Values.conductor.redis_rack }}
    {{- end -}}

    # System task worker
    workflow.system.task.worker.callback.seconds=30
    workflow.system.task.worker.thread.count=5
    # Number of items to poll for
    workflow.system.task.worker.poll.count=5
    # Interval in ms at which the polling is done
    workflow.system.task.worker.poll.interval=20
    workflow.system.task.worker.queue.size=100

    # Workflow monitor
    workflow.monitor.metadata.refresh.counter=10
    workflow.monitor.stats.freq.seconds=60

    # Workflow Sweeper
    workflow.sweeper.thread.count=5

    # EventProcessor
    workflow.event.processor.thread.count=2

    # RedisMetadataDAO
    conductor.taskdef.cache.refresh.time.seconds=60

    # ExecutionService (requeuePendingTasks)
    task.requeue.timeout=60_000
{{- end }}
